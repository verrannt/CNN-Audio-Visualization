2017-06-20_r2
*

2017-06-20_r3
* conv1-conv2-pool-fc6-dropout-fc7
* keep_prob = 0.5
* batch_size = 50
* act = leaky relu
* weights_stddev = 0.3
* learning_rate = 0.1

2017-06-20_r4
* conv1-conv2-conv3-pool-fc6-dp-fc7
* keep_prob = 1.0
* batch_size = 100
* act = leaky relu
* weights_stddev = 0.1
* learning_rate = 0.05

2017-06-20_r5
* conv1-conv2-pool-dp-conv3-conv4-pool4-fc1-dp-fc2
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.2 wie alle bis jetzt)
* weights_stddev = 0.1
* learning_rate = 0.01

2017-06-21_r1
* conv1-conv2-pool-dp-conv3-conv4-pool4-conv5-conv5_1-conv5_2-fc1-dp-fc2
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.01)
* weights_stddev = 0.1
* learning_rate = 0.01

2017-06-21_r2
* conv1-conv2-pool-dp-conv3-6-pool6-conv7-9-fc1-dp-fc2
* layers conv3 to conv9 now use 64-kernels
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.01)
* weights_stddev = 0.1
* learning_rate = 0.005

2017-06-21_r3
* conv1-conv2-pool-dp-conv3-6-pool6-conv7-9-fc1-dp-fc2
* layers conv3 to conv9 now use 64-kernels
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.2)
* weights_stddev = 0.1
* learning_rate = 0.01

2017-06-21_r4
* conv1-conv2-pool-dp-conv3-conv4-pool4-fc1-dp-fc2
* arc = fat_shallow
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.2)
* weights_stddev = 0.1
* learning_rate = 0.01

2017-06-22_r1
* conv1-conv2-pool-dp-conv3-conv4-pool4-fc1-fc2
* arc = fat_shallow
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.2)
* weights_stddev = 0.1
* learning_rate = 0.01

2017-06-23_r1
* conv1-conv2-pool-dp-conv3-conv4-pool4-fc1-fc1_1-fc2
* arc = fat_shallow
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.2)
* weights_stddev = 0.1
* learning_rate = 0.01

2017-06-23_r2
* conv1-conv2-pool-dp-conv3-conv4-pool4-fc1-dp-fc1_1-fc2
* arc = fat_shallow
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.2)
* weights_stddev = 0.8
* learning_rate = 0.03

2017-06-27_r1
* conv1-conv2-pool-dp-conv3-conv4-conv4_1-pool4-fc1-dp-fc1_1-fc2
* arc = fat_shallow
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.2)
* weights_stddev = 0.8
* learning_rate = 0.03

2017-06-29_r1
* conv1-conv2-pool-dp-conv3-conv4-pool4-fc1-dp-fc1_1-fc2
* arc = fat_shallow
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.2)
* weights_stddev = 0.8
* learning_rate = 0.03
* init biases random normal now! with different stddev:
* bias_conv_stddev = 1.0
* bias_fc_stddev = 10.0

2017-07-02_r1 kurz nach mitternacht
* repo gestashed, neu gepulled und nur conv_1 auf 128 gescaled.
* sonst sollte der code derselbe wie f√ºr floyd sein
* log ist in dem ordner in tensorboard muss gemoved werden

2017-07-05
* conv1-conv2-pool-dp-conv3-conv4-pool4-fc1-dp-fc2
* arc = fat_shallow
* keep_prob = 0.7
* batch_size = 100
* act = leaky relu (0.2)
* weights_stddev = 0.8
* learning_rate = 0.03
* bias is constant_init(0.1) again

2017-07-25
* conv1-conv2-pool-dp-conv3-conv4-pool4-fc1-dp-fc2
* arc = fat_shallow
* keep_prob = 0.5
* batch_size = 100
* act = leaky relu (0.2)
* weights_stddev = 0.8
* learning_rate = 0.01 dynamic
